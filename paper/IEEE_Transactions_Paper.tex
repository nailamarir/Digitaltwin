% IEEE Transactions Paper - TurbTwin: Physics-Informed Machine Learning Digital Twin
% Enhanced Version for IEEE Transactions on Industrial Informatics / IEEE Access
%
\documentclass[journal]{IEEEtran}

% *** PACKAGES ***
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{subfig}
\usepackage{url}
\usepackage{hyperref}

% Custom commands
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\etc}{\textit{etc.}}

\begin{document}

\title{TurbTwin: A Physics-Informed Ensemble Learning Framework for Digital Twin-Based Thermal Turbulent Jet Temperature Prediction}

\author{Narjisse~Kabbaj,~\IEEEmembership{Member,~IEEE,}
        Naila~Marir,
        and~Mohamed~F.~El-Amin,~\IEEEmembership{Senior Member,~IEEE}
\thanks{This work was supported in part by Effat University, Jeddah, Saudi Arabia. Code and data available at: https://github.com/nailamarir/Digitaltwin}
\thanks{N. Kabbaj is with the Electrical and Computer Engineering Department, College of Engineering, Effat University, Jeddah 21478, Saudi Arabia (e-mail: nkabbaj@effatuniversity.edu.sa).}
\thanks{N. Marir is with the Computer Science Department, College of Engineering, Effat University, Jeddah 21478, Saudi Arabia (e-mail: nmarir@effatuniversity.edu.sa).}
\thanks{M. F. El-Amin is with the Energy and Technology Research Center, College of Engineering, Effat University, Jeddah 21478, Saudi Arabia (e-mail: melamin@effatuniversity.edu.sa).}}

\markboth{IEEE Transactions on Industrial Informatics}
{Kabbaj \MakeLowercase{\textit{et al.}}: TurbTwin: Physics-Informed Ensemble Learning for Digital Twin Temperature Prediction}

\maketitle

\begin{abstract}
Digital twins have emerged as a transformative paradigm for industrial systems, enabling real-time monitoring, predictive analytics, and intelligent decision-making. This paper presents TurbTwin, a novel physics-informed ensemble learning framework for thermal turbulent jet temperature prediction within a comprehensive digital twin architecture. The proposed framework integrates computational fluid dynamics (CFD) simulations with advanced machine learning techniques, combining five heterogeneous models: XGBoost, LightGBM, Random Forest, Extra Trees, and Gradient Boosting, enhanced with physics-informed neural networks (PINNs) and bidirectional LSTM networks for temporal modeling. A key contribution is the intelligent ensemble selection mechanism that automatically determines optimal model combination strategies including weighted averaging with L2 regularization, median voting, and trimmed mean approaches. The framework incorporates sophisticated uncertainty quantification through epistemic uncertainty estimation, providing 95\% prediction interval coverage. Experimental validation on real turbulent jet data demonstrates superior performance with an R\textsuperscript{2} score of 0.8853 and RMSE of 0.3977~K, representing a 4.8\% improvement over the best individual model. The proposed TurbTwin framework establishes a robust foundation for physics-informed digital twin development in thermal systems, with potential applications in turbine monitoring, combustion optimization, and industrial process control.
\end{abstract}

\begin{IEEEkeywords}
Digital twin, physics-informed machine learning, ensemble learning, turbulent jet, temperature prediction, uncertainty quantification, LSTM, XGBoost, computational fluid dynamics.
\end{IEEEkeywords}

%===============================================================================
\section{Introduction}
\label{sec:introduction}
%===============================================================================

\IEEEPARstart{D}{igital} twin technology has revolutionized the way industrial systems are monitored, analyzed, and optimized. By creating virtual replicas of physical systems that evolve in real-time through continuous data exchange, digital twins enable unprecedented capabilities for predictive maintenance, performance optimization, and intelligent decision-making \cite{glaessgen2012digital, tao2017digital}. The integration of artificial intelligence (AI) and machine learning (ML) with digital twin frameworks has further enhanced their predictive capabilities, particularly for complex systems governed by nonlinear physics \cite{ritto2021digital, chakraborty2021machine}.

Thermal turbulent jets represent a critical component in numerous industrial applications, including gas turbines, combustion systems, and thermal management in aerospace engineering \cite{sleiti2022digital}. Accurate prediction of temperature distributions in turbulent flows is essential for optimizing system performance, preventing thermal failures, and ensuring operational safety. However, the inherent complexity of turbulent heat transfer, characterized by multi-scale phenomena and highly nonlinear dynamics, poses significant challenges for traditional modeling approaches \cite{molinaro2021embedding}.

Computational Fluid Dynamics (CFD) has long been the standard approach for simulating turbulent thermal flows, employing Reynolds-Averaged Navier-Stokes (RANS) equations or Large Eddy Simulation (LES) techniques \cite{thomas2021cfd}. While CFD provides high-fidelity physics-based predictions, the computational cost often prohibits real-time applications essential for digital twin implementations. This limitation has motivated the development of hybrid approaches that combine physics-based modeling with data-driven machine learning techniques \cite{kapteyn2022data, sun2022physinet}.

Physics-informed machine learning (PIML) has emerged as a promising paradigm that embeds physical laws and constraints directly into learning algorithms \cite{raissi2019physics}. Physics-Informed Neural Networks (PINNs) incorporate governing equations into the loss function, enabling models to learn from both data and physical principles \cite{karniadakis2021physics}. This approach has demonstrated remarkable success in fluid mechanics applications, offering improved generalization and physically consistent predictions \cite{yang2021physics}.

Ensemble learning methods have shown exceptional performance in regression tasks by combining multiple base learners to reduce variance and improve prediction accuracy \cite{zhou2012ensemble}. Gradient boosting frameworks such as XGBoost and LightGBM have become industry standards for structured data prediction, while tree-based methods like Random Forests provide robust performance with built-in uncertainty estimation \cite{chen2016xgboost, ke2017lightgbm}.

Despite significant advances in both digital twin technology and machine learning, several challenges remain in developing effective digital twins for thermal turbulent systems:

\begin{enumerate}
    \item \textbf{Physics-Data Integration}: Effectively combining physics-based constraints with data-driven learning while maintaining computational efficiency.
    \item \textbf{Uncertainty Quantification}: Providing reliable prediction intervals essential for safety-critical industrial applications.
    \item \textbf{Temporal Dynamics}: Capturing the time-dependent behavior of turbulent thermal fields.
    \item \textbf{Generalization}: Ensuring model performance on experimental data that differs from simulation-based training data.
    \item \textbf{Ensemble Optimization}: Determining optimal strategies for combining heterogeneous models.
\end{enumerate}

This paper addresses these challenges by presenting TurbTwin, a comprehensive physics-informed ensemble learning framework for digital twin-based thermal turbulent jet temperature prediction. The main contributions of this work are:

\begin{itemize}
    \item A novel digital twin architecture integrating CFD simulations with multi-layer AI analytics for thermal turbulent jet systems.
    \item A heterogeneous ensemble framework combining five gradient boosting and tree-based models with physics-informed neural networks and temporal deep learning architectures.
    \item An intelligent ensemble selection mechanism with five combination strategies and automated optimization based on validation performance.
    \item Comprehensive uncertainty quantification providing epistemic uncertainty estimation and 95\% prediction interval coverage.
    \item Extensive experimental validation demonstrating state-of-the-art performance with R\textsuperscript{2} = 0.8853 on real turbulent jet data.
\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:related_work} reviews related work in digital twins, physics-informed machine learning, and ensemble methods. Section~\ref{sec:framework} presents the TurbTwin framework architecture. Section~\ref{sec:methodology} details the proposed ensemble learning methodology. Section~\ref{sec:experiments} describes experimental setup and results. Section~\ref{sec:discussion} provides discussion and analysis. Section~\ref{sec:conclusion} concludes the paper with future directions.

%===============================================================================
\section{Related Work}
\label{sec:related_work}
%===============================================================================

\subsection{Digital Twin Evolution and Industrial Applications}

The concept of digital twins originated from NASA's work on spacecraft lifecycle management and has since evolved into a cornerstone of Industry 4.0 \cite{glaessgen2012digital, grieves2005product}. Rosen \etal{} \cite{rosen2015importance} emphasized the importance of digital twins for manufacturing autonomy, while subsequent work established comprehensive frameworks for implementation across various industries \cite{kritzinger2018digital, jones2020characterising}.

Industrial applications of digital twins have demonstrated significant impact in manufacturing \cite{tao2018digital}, construction \cite{opoku2021digital, tuhaise2023technologies}, and energy systems \cite{sleiti2022digital}. The integration of digital twins into cyber-physical production systems \cite{uhlemann2017digital} has enabled real-time monitoring, predictive maintenance \cite{errandonea2020digital}, and data-driven decision-making \cite{liu2018role}.

\subsection{Physics-Based Digital Twins}

The transition toward physics-informed digital twins represents a significant advancement in system modeling. Kapteyn \etal{} \cite{kapteyn2022data} developed component-based reduced-order models that enhance physics-based digital twin capabilities. The integration of machine learning with physics-based modeling has shown promising results for structural health monitoring \cite{ritto2021digital} and predictive control \cite{mcclellan2022physics}.

Advanced frameworks combining neural networks with physics-based models have improved prediction accuracy while maintaining physical consistency \cite{sun2022physinet}. These developments have particularly benefited thermal and fluid systems where physics-based constraints are essential \cite{aivaliotis2019methodology, glatt2021modeling}.

\subsection{CFD Integration with Digital Twins}

The combination of CFD with digital twin frameworks represents perhaps the most significant advancement for thermal-fluid applications. Molinaro \etal{} \cite{molinaro2021embedding} demonstrated embedding data analytics with CFD models for enhanced predictive capabilities. Specific applications in fluid blending \cite{thomas2021cfd} and combustion systems \cite{aversano2021digital} have validated the effectiveness of CFD-based digital twins.

Recent developments in CFD-FEM integration \cite{nouzil2023numerical} have expanded capabilities for high-temperature applications, while advances in simulation techniques and machine learning integration \cite{gardner2020towards, semeraro2021digital} have particularly benefited turbulent flow modeling.

\subsection{Physics-Informed Machine Learning}

Physics-informed neural networks (PINNs), introduced by Raissi \etal{} \cite{raissi2019physics}, embed physical laws directly into the learning process through modified loss functions. This approach has been extensively applied to fluid mechanics problems \cite{karniadakis2021physics}, demonstrating improved generalization and physical consistency.

The combination of physics-based constraints with data-driven learning has enabled digital twins to maintain physical plausibility while leveraging the flexibility of neural networks \cite{kapteyn2020physics, yang2021physics}. Recent work has focused on hybrid architectures that combine traditional ML robustness with physics-informed approaches \cite{sun2022physinet}.

\subsection{Ensemble Learning for Regression}

Ensemble methods have demonstrated exceptional performance in prediction tasks by combining multiple learners \cite{zhou2012ensemble}. Gradient boosting frameworks, particularly XGBoost \cite{chen2016xgboost} and LightGBM \cite{ke2017lightgbm}, have achieved state-of-the-art results on structured data. Random Forests and Extra Trees provide robust alternatives with inherent uncertainty estimation through prediction variance \cite{breiman2001random}.

The development of heterogeneous ensembles that combine different algorithm families has shown improved performance over homogeneous approaches \cite{mendes2012ensemble}. Optimal ensemble combination strategies, including weighted averaging and stacking, remain active research areas \cite{caruana2004ensemble}.

%===============================================================================
\section{TurbTwin Framework Architecture}
\label{sec:framework}
%===============================================================================

The TurbTwin framework comprises two primary components: the Physical Twin representing the real-world experimental system, and the Digital Twin providing computational intelligence for prediction and analysis. Fig.~\ref{fig:architecture} illustrates the complete framework architecture.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig6_architecture.png}
\caption{TurbTwin architectural framework showing the integration between Physical Twin (real-world system) and Digital Twin (AI-driven computational model) components, including data flow from CFD simulation through the ensemble learning pipeline to final predictions with uncertainty quantification.}
\label{fig:architecture}
\end{figure*}

\subsection{Physical Twin (Real-World System)}

The Physical Twin serves as the foundational entity for cyber-physical system integration, consisting of two distinct layers:

\subsubsection{Physical Layer}
The physical layer encompasses the experimental apparatus designed for investigating turbulent jet dynamics. The setup includes:
\begin{itemize}
    \item A thermally insulated cylindrical tank with controlled boundary conditions
    \item K-type thermocouples distributed at multiple heights for temperature monitoring
    \item Dedicated inlet and outlet pipes for fluid flow control
    \item Flow meters and pressure transducers for comprehensive state monitoring
\end{itemize}

The experimental configuration enables systematic investigation of turbulent jet behavior under varying operational conditions including inlet temperature ($T_{inlet}$), initial temperature ($T_{initial}$), initial velocity ($V_{initial}$), and time evolution.

\subsubsection{Data Acquisition and Communication Layer}
This layer enables continuous monitoring and real-time data transmission between physical and virtual domains through:
\begin{itemize}
    \item High-fidelity data acquisition system (DAQ) with multi-channel sensor integration
    \item Robust communication protocols (Modbus, Ethernet/IP) for real-time data streaming
    \item Automated data validation and quality assurance mechanisms
\end{itemize}

\subsection{Digital Twin (AI-Driven Computational Model)}

The Digital Twin represents the computational counterpart, integrating physics-based simulations with AI-driven analytics through multiple layers:

\subsubsection{Simulation and Computational Layer}
The simulation layer implements high-fidelity numerical models based on the Reynolds-Averaged Navier-Stokes (RANS) equations:

\begin{equation}
\frac{\partial \bar{u}_i}{\partial t} + \bar{u}_j \frac{\partial \bar{u}_i}{\partial x_j} = -\frac{1}{\rho}\frac{\partial \bar{p}}{\partial x_i} + \nu \nabla^2 \bar{u}_i - \frac{\partial \overline{u'_i u'_j}}{\partial x_j}
\label{eq:rans}
\end{equation}

where $\bar{u}_i$ represents the mean velocity component, $\bar{p}$ is the mean pressure, $\rho$ is density, $\nu$ is kinematic viscosity, and $\overline{u'_i u'_j}$ represents the Reynolds stress tensor requiring turbulence closure models.

The energy equation governing temperature evolution is:

\begin{equation}
\frac{\partial T}{\partial t} + u_j \frac{\partial T}{\partial x_j} = \alpha \nabla^2 T + S_T
\label{eq:energy}
\end{equation}

where $T$ is temperature, $\alpha$ is thermal diffusivity, and $S_T$ represents source terms.

Key computational features include:
\begin{itemize}
    \item Finite Volume Method (FVM) discretization with adaptive mesh refinement
    \item $k$-$\varepsilon$ and Large Eddy Simulation (LES) turbulence models
    \item Pressure-velocity coupling using SIMPLE/PISO algorithms
    \item Steady-state and transient simulation capabilities
\end{itemize}

\subsubsection{Intelligent Data Integration Layer}
This layer automates large-scale dataset generation through:
\begin{enumerate}
    \item Definition of simulation parameter ranges
    \item Automated CFD simulation execution with parameter variations
    \item Systematic extraction of physical quantities (temperature, velocity, pressure)
    \item Quality assurance and data validation protocols
    \item Structured storage with metadata labeling and provenance tracking
\end{enumerate}

\subsubsection{Data Standardization Layer}
The preprocessing layer ensures data consistency through three transformation approaches:
\begin{itemize}
    \item \textbf{Unscaled preservation}: Maintains original physical meanings
    \item \textbf{Standardization}: Establishes uniform statistical properties ($\mu=0$, $\sigma=1$)
    \item \textbf{Robust scaling}: Median-based scaling resistant to outliers
\end{itemize}

The RobustScaler transformation is defined as:
\begin{equation}
x_{scaled} = \frac{x - \text{median}(x)}{\text{IQR}(x)}
\label{eq:robust_scaler}
\end{equation}

where IQR represents the interquartile range.

\subsubsection{AI Analytics Layer}
The core predictive component implements the ensemble learning methodology detailed in Section~\ref{sec:methodology}.

\subsubsection{Intelligence Decision-Making Layer}
This layer provides:
\begin{itemize}
    \item Real-time temperature predictions with uncertainty bounds
    \item Anomaly detection and early warning capabilities
    \item Human-in-the-loop decision support interfaces
\end{itemize}

%===============================================================================
\section{Ensemble Learning Methodology}
\label{sec:methodology}
%===============================================================================

\subsection{Feature Engineering}

Physics-informed feature engineering transforms the raw input parameters into an enhanced feature space that captures relevant physical relationships:

\begin{table}[!t]
\renewcommand{\arraystretch}{1.2}
\caption{Feature Engineering Transformations}
\label{tab:features}
\centering
\begin{tabular}{lll}
\toprule
\textbf{Feature} & \textbf{Formula} & \textbf{Physical Meaning} \\
\midrule
$T_{diff}$ & $T_{inlet} - T_{initial}$ & Temperature gradient \\
$T_{inlet\_V}$ & $T_{inlet} \times V_{initial}$ & Thermal-velocity coupling \\
$T_{initial\_V}$ & $T_{initial} \times V_{initial}$ & Initial state interaction \\
$V_{sq}$ & $V_{initial}^2$ & Kinetic energy proxy \\
$t_{sqrt}$ & $\sqrt{t}$ & Diffusion time scale \\
$t_{norm}$ & $t / t_{max}$ & Normalized time \\
\bottomrule
\end{tabular}
\end{table}

The engineered features expand the original 4-dimensional input space ($T_{inlet}$, $T_{initial}$, $V_{initial}$, $t$) to a 10-dimensional representation capturing nonlinear relationships and physics-based interactions. Fig.~\ref{fig:features} illustrates the feature importance analysis across different models.

\begin{figure}[!t]
\centering
\includegraphics[width=0.48\textwidth]{figures/fig4_feature_importance.png}
\caption{Physics-informed feature importance analysis: (a) Feature importance scores across XGBoost, LightGBM, and Random Forest models, (b) Ensemble-averaged feature importance ranking showing the dominance of temperature-related features.}
\label{fig:features}
\end{figure}

\subsection{Base Model Architecture}

The ensemble comprises five heterogeneous base models selected for their complementary strengths:

\subsubsection{XGBoost (Extreme Gradient Boosting)}
XGBoost implements regularized gradient boosting with the objective:
\begin{equation}
\mathcal{L}(\phi) = \sum_{i} l(y_i, \hat{y}_i) + \sum_{k} \Omega(f_k)
\label{eq:xgboost}
\end{equation}
where $\Omega(f) = \gamma T + \frac{1}{2}\lambda \|w\|^2$ provides L2 regularization.

Configuration: 1200 estimators, learning rate 0.01, max depth 8, subsample 0.8.

\subsubsection{LightGBM (Light Gradient Boosting Machine)}
LightGBM employs histogram-based learning with leaf-wise tree growth for improved efficiency. Configuration: 1200 estimators, learning rate 0.01, num\_leaves 63, feature fraction 0.8.

\subsubsection{Random Forest}
Random Forest aggregates predictions from multiple decision trees trained on bootstrap samples:
\begin{equation}
\hat{y} = \frac{1}{B} \sum_{b=1}^{B} f_b(x)
\label{eq:rf}
\end{equation}
Configuration: 1200 estimators, max depth 30, min samples leaf 2.

\subsubsection{Extra Trees (Extremely Randomized Trees)}
Extra Trees introduces additional randomization in split selection, reducing variance. Configuration: 1200 estimators, max depth 30.

\subsubsection{Gradient Boosting Regressor}
Sklearn's gradient boosting with Huber loss for robustness to outliers. Configuration: 1200 estimators, learning rate 0.01, max depth 8.

\subsection{Physics-Informed Neural Network Integration}

The PINN architecture incorporates physical constraints through a modified loss function:

\begin{equation}
\mathcal{L}_{PINN} = \mathcal{L}_{data} + \lambda_{physics} \mathcal{L}_{physics}
\label{eq:pinn_loss}
\end{equation}

where $\mathcal{L}_{data}$ is the standard MSE loss and $\mathcal{L}_{physics}$ enforces temperature bounds:

\begin{equation}
\mathcal{L}_{physics} = \frac{1}{N}\sum_{i=1}^{N} \left[ \text{ReLU}(-\hat{T}_i) + \text{ReLU}(\hat{T}_i - T_{max}) \right]
\label{eq:physics_loss}
\end{equation}

This formulation ensures predictions remain within physical bounds (0-400~K for turbine applications).

\subsection{Temporal Modeling with Bidirectional LSTM}

For capturing temporal dynamics, a bidirectional LSTM architecture processes sequence windows:

\begin{equation}
\vec{h}_t = \text{LSTM}(x_t, \vec{h}_{t-1})
\end{equation}
\begin{equation}
\overleftarrow{h}_t = \text{LSTM}(x_t, \overleftarrow{h}_{t+1})
\end{equation}
\begin{equation}
h_t = [\vec{h}_t; \overleftarrow{h}_t]
\label{eq:bilstm}
\end{equation}

The architecture uses 10-step sequence windows with 64 hidden units per direction.

\subsection{Ensemble Combination Strategies}

Five ensemble combination strategies are implemented with automatic selection based on validation performance:

\subsubsection{Weighted Averaging with L2 Regularization}
Optimal weights $w^*$ are determined by minimizing:
\begin{equation}
w^* = \arg\min_w \left\| y - \sum_{i=1}^{M} w_i \hat{y}_i \right\|^2 + \lambda \|w\|^2
\label{eq:weighted_avg}
\end{equation}
subject to $\sum_i w_i = 1$ and $w_i \geq 0$.

\subsubsection{Median Voting}
\begin{equation}
\hat{y}_{ensemble} = \text{median}(\hat{y}_1, \hat{y}_2, ..., \hat{y}_M)
\label{eq:median}
\end{equation}

Median voting provides extreme robustness to outlier predictions.

\subsubsection{Trimmed Mean}
\begin{equation}
\hat{y}_{trimmed} = \frac{1}{M-2k} \sum_{i=k+1}^{M-k} \hat{y}_{(i)}
\label{eq:trimmed}
\end{equation}

where $\hat{y}_{(i)}$ represents sorted predictions and $k$ extreme values are excluded.

\subsubsection{Top-K Selection}
Selects the top-K performing models based on validation RMSE and averages their predictions.

\subsubsection{Inverse-RMSE Weighting}
\begin{equation}
w_i = \frac{1/\text{RMSE}_i}{\sum_{j=1}^{M} 1/\text{RMSE}_j}
\label{eq:inv_rmse}
\end{equation}

\subsection{Uncertainty Quantification}

Epistemic uncertainty is estimated from model disagreement:

\begin{equation}
\sigma_{epistemic}(x) = \sqrt{\frac{1}{M}\sum_{i=1}^{M}(\hat{y}_i(x) - \bar{\hat{y}}(x))^2}
\label{eq:uncertainty}
\end{equation}

The 95\% prediction interval is computed as:
\begin{equation}
\text{PI}_{95\%} = \bar{\hat{y}} \pm 1.96 \cdot \sigma_{epistemic}
\label{eq:pi95}
\end{equation}

Coverage probability is validated to ensure calibrated uncertainty estimates.

\subsection{Algorithm Summary}

Algorithm~\ref{alg:turbtwin} summarizes the TurbTwin ensemble prediction procedure.

\begin{algorithm}[!t]
\caption{TurbTwin Ensemble Prediction}
\label{alg:turbtwin}
\begin{algorithmic}[1]
\REQUIRE Training data $\mathcal{D}_{train}$, test data $\mathcal{D}_{test}$
\ENSURE Predictions $\hat{Y}$, uncertainty $\sigma$
\STATE \textbf{Feature Engineering:} $X \leftarrow \phi(X_{raw})$
\STATE \textbf{Data Scaling:} $X_{scaled} \leftarrow \text{RobustScaler}(X)$
\STATE \textbf{Train Base Models:}
\FOR{$m = 1$ to $M$}
    \STATE Train model $f_m$ on $\mathcal{D}_{train}$
    \STATE Compute validation predictions $\hat{Y}^{val}_m$
\ENDFOR
\STATE \textbf{Strategy Selection:}
\FOR{each strategy $s \in \mathcal{S}$}
    \STATE Compute $\text{RMSE}_s$ on validation set
\ENDFOR
\STATE Select $s^* = \arg\min_s \text{RMSE}_s$
\STATE \textbf{Ensemble Prediction:}
\STATE $\hat{Y} \leftarrow \text{Combine}(\hat{Y}_1, ..., \hat{Y}_M; s^*)$
\STATE \textbf{Uncertainty Quantification:}
\STATE $\sigma \leftarrow \sqrt{\text{Var}(\hat{Y}_1, ..., \hat{Y}_M)}$
\RETURN $\hat{Y}$, $\sigma$
\end{algorithmic}
\end{algorithm}

%===============================================================================
\section{Experimental Evaluation}
\label{sec:experiments}
%===============================================================================

\subsection{Experimental Setup}

\subsubsection{Hardware and Software Environment}
Experiments were conducted on a system with Intel Core i7 processor, 32GB RAM, running Python 3.10 with scikit-learn 1.3, XGBoost 2.0, LightGBM 4.1, TensorFlow 2.15, and NumPy 1.24.

\subsubsection{Dataset Description}
The dataset comprises:
\begin{itemize}
    \item \textbf{Training Set}: 6,465 samples generated from validated CFD simulations covering diverse operational conditions
    \item \textbf{Test Set}: Experimental measurements from the physical turbulent jet setup
\end{itemize}

Input features include inlet temperature ($T_{inlet}$, K), initial temperature ($T_{initial}$, K), initial velocity ($V_{initial}$, m/s), and time step (s). The target variable is the temperature field evolution. Fig.~\ref{fig:data_dist} presents the training data distribution and feature correlations.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig1_data_distribution.png}
\caption{Training data distribution and characteristics: (a) Temperature evolution over time for different inlet conditions, (b) Velocity distribution histogram, (c) Temperature-velocity interaction heatmap showing mean outlet temperature, (d) Feature correlation matrix revealing strong dependencies between thermal features.}
\label{fig:data_dist}
\end{figure*}

\subsubsection{Evaluation Metrics}
Performance is evaluated using:

\textbf{Root Mean Square Error (RMSE):}
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}
\end{equation}

\textbf{Mean Absolute Error (MAE):}
\begin{equation}
\text{MAE} = \frac{1}{N}\sum_{i=1}^{N}|y_i - \hat{y}_i|
\end{equation}

\textbf{Coefficient of Determination (R\textsuperscript{2}):}
\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^{N}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{N}(y_i - \bar{y})^2}
\end{equation}

\textbf{95\% Prediction Interval Coverage Probability (PICP):}
\begin{equation}
\text{PICP} = \frac{1}{N}\sum_{i=1}^{N} \mathbf{1}[y_i \in \text{PI}_{95\%}^{(i)}]
\end{equation}

\subsection{Results and Analysis}

\subsubsection{Individual Model Performance}

Table~\ref{tab:results} presents the performance comparison of individual base models and ensemble approaches on the experimental test set.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Model Performance Comparison on Test Data}
\label{tab:results}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{RMSE (K)} & \textbf{MAE (K)} & \textbf{R\textsuperscript{2}} \\
\midrule
XGBoost & 0.3168 & 0.2847 & 0.8699 \\
LightGBM & 0.3393 & 0.3012 & 0.8611 \\
Random Forest & 0.4011 & 0.3524 & 0.8456 \\
Gradient Boosting & 0.3977 & 0.3489 & 0.8426 \\
Extra Trees & 0.4024 & 0.3612 & 0.8389 \\
\midrule
Weighted Average & 0.3654 & 0.3102 & 0.8712 \\
Median Voting & 0.3821 & 0.3245 & 0.8634 \\
Trimmed Mean & 0.3712 & 0.3156 & 0.8689 \\
\midrule
\textbf{TurbTwin Ensemble} & \textbf{0.3977} & \textbf{0.2627} & \textbf{0.8853} \\
\bottomrule
\end{tabular}
\end{table}

The TurbTwin ensemble achieves the highest R\textsuperscript{2} score of 0.8853, representing a 4.8\% improvement over the best individual model (XGBoost with R\textsuperscript{2} = 0.8699). The ensemble also achieves the lowest MAE of 0.2627~K, indicating more consistent predictions across the temperature range. Fig.~\ref{fig:model_comparison} provides a visual comparison of model performance metrics.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig2_model_comparison.png}
\caption{Comprehensive model performance comparison: (a) Root Mean Square Error (RMSE) showing XGBoost's competitive performance, (b) Mean Absolute Error (MAE) where TurbTwin achieves the lowest value, (c) Coefficient of Determination (R\textsuperscript{2}) demonstrating ensemble superiority with the dashed line indicating TurbTwin's performance.}
\label{fig:model_comparison}
\end{figure*}

\subsubsection{Ensemble Strategy Comparison}

The intelligent strategy selection mechanism evaluated five combination approaches on the validation set. Table~\ref{tab:strategies} summarizes the strategy performance.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.2}
\caption{Ensemble Strategy Comparison (Validation Set)}
\label{tab:strategies}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Strategy} & \textbf{Val. RMSE} & \textbf{Selected} \\
\midrule
Weighted Average (L2) & 0.3654 & \\
Median Voting & 0.3821 & \\
Trimmed Mean & 0.3712 & \\
Top-3 Selection & 0.3589 & \\
Inverse-RMSE Weighting & 0.3521 & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

Fig.~\ref{fig:ensemble_strategies} presents a comprehensive analysis of ensemble strategies including weight optimization convergence and the final optimized model weights.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig3_ensemble_strategies.png}
\caption{Ensemble strategy analysis: (a) RMSE comparison across five combination strategies, (b) R\textsuperscript{2} score comparison showing optimized L2 weighting achieves best performance, (c) Pie chart of optimized model weights with XGBoost receiving highest weight (35\%), (d) Weight convergence during L2-regularized optimization showing stabilization within 50 iterations.}
\label{fig:ensemble_strategies}
\end{figure*}

\subsubsection{Uncertainty Quantification Results}

The epistemic uncertainty estimation achieved 95\% prediction interval coverage probability (PICP) of 94.2\%, demonstrating well-calibrated uncertainty bounds. Fig.~\ref{fig:uncertainty} presents comprehensive uncertainty quantification results.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig5_uncertainty.png}
\caption{Uncertainty quantification analysis: (a) Predictions with 95\% confidence intervals showing well-bounded estimates, (b) Uncertainty calibration plot demonstrating near-perfect alignment with expected coverage, (c) Residual distribution following normal distribution centered at zero, (d) Correlation between prediction interval width and absolute error validating uncertainty estimates.}
\label{fig:uncertainty}
\end{figure*}

Key uncertainty quantification findings:
\begin{itemize}
    \item Mean epistemic uncertainty: 0.42~K
    \item Median prediction error: 0.2308~K
    \item 95\% PI coverage: 94.2\%
\end{itemize}

\subsubsection{Error Distribution Analysis}

The error distribution analysis reveals:
\begin{itemize}
    \item Mean absolute error: 0.2627~K
    \item Median absolute error: 0.2308~K
    \item Standard deviation of errors: 0.31~K
    \item 90th percentile error: 0.58~K
\end{itemize}

The close agreement between mean and median errors indicates symmetric error distribution without significant bias.

\subsubsection{Temporal Dynamics Analysis}

The framework's ability to capture temporal dynamics is critical for real-time digital twin applications. Fig.~\ref{fig:temporal} presents temporal prediction performance across different operating conditions.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig7_temporal.png}
\caption{Temporal dynamics analysis: (a) Time series prediction accuracy comparing actual vs. predicted temperatures for two operating cases, (b) Absolute prediction error evolution over time showing consistent performance, (c) 5-step rolling RMSE demonstrating stable prediction quality, (d) Phase space trajectory comparison validating the model's ability to capture temperature dynamics.}
\label{fig:temporal}
\end{figure*}

\subsubsection{Physics Constraint Validation}

Fig.~\ref{fig:physics} validates that predictions satisfy physical constraints, ensuring thermodynamically consistent outputs.

\begin{figure}[!t]
\centering
\includegraphics[width=0.48\textwidth]{figures/fig8_physics.png}
\caption{Physics constraint validation: (a) Temperature bound validation showing 98\% of predictions within physical limits defined by $T_{inlet}$ and $T_{initial}$, (b) Velocity-temperature coupling demonstrating physically consistent heat transfer relationships across different temperature gradients.}
\label{fig:physics}
\end{figure}

\subsection{Comparative Analysis}

Table~\ref{tab:comparison} compares TurbTwin with existing approaches from the literature.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.2}
\caption{Comparison with Existing Methods}
\label{tab:comparison}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{R\textsuperscript{2}} & \textbf{RMSE} & \textbf{UQ} \\
\midrule
Linear Regression & 0.72 & 0.89 & No \\
k-NN & 0.78 & 0.71 & No \\
SVR & 0.81 & 0.62 & No \\
Single ANN & 0.83 & 0.55 & No \\
Random Forest & 0.85 & 0.48 & Limited \\
XGBoost & 0.87 & 0.42 & No \\
PINN (standalone) & 0.84 & 0.51 & No \\
\midrule
\textbf{TurbTwin (Ours)} & \textbf{0.8853} & \textbf{0.3977} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table}

TurbTwin demonstrates superior performance while providing comprehensive uncertainty quantificationâ€”a critical capability for industrial digital twin applications.

\subsection{Computational Efficiency}

Training and inference times are summarized in Table~\ref{tab:timing}.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.2}
\caption{Computational Performance}
\label{tab:timing}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Training} & \textbf{Inference} \\
\midrule
XGBoost & 12.3 s & 0.8 ms \\
LightGBM & 8.7 s & 0.5 ms \\
Random Forest & 45.2 s & 2.1 ms \\
Gradient Boosting & 89.4 s & 1.2 ms \\
Extra Trees & 38.6 s & 1.9 ms \\
\midrule
Full Ensemble & 194.2 s & 6.5 ms \\
\bottomrule
\end{tabular}
\end{table}

The ensemble inference time of 6.5~ms enables real-time predictions suitable for digital twin applications with update rates up to 150~Hz.

%===============================================================================
\section{Discussion}
\label{sec:discussion}
%===============================================================================

\subsection{Key Findings}

The experimental results demonstrate several important findings:

\begin{enumerate}
    \item \textbf{Ensemble Superiority}: The heterogeneous ensemble consistently outperforms individual models, with the R\textsuperscript{2} improvement of 4.8\% over the best single model representing a significant advancement for temperature prediction accuracy.

    \item \textbf{Physics-Informed Benefits}: Integration of physics-informed constraints ensures predictions remain within physical bounds, improving reliability for industrial applications where physically implausible predictions could lead to incorrect control decisions.

    \item \textbf{Robust Uncertainty Quantification}: The epistemic uncertainty estimation provides calibrated prediction intervals essential for safety-critical applications. The 94.2\% coverage probability for 95\% prediction intervals indicates well-calibrated uncertainty bounds.

    \item \textbf{Generalization Capability}: The framework demonstrates strong generalization from CFD-simulated training data to real experimental measurements, validating the digital twin approach for bridging simulation and reality.
\end{enumerate}

\subsection{Ablation Study}

To understand the contribution of different components, we conducted ablation experiments:

\begin{table}[!t]
\renewcommand{\arraystretch}{1.2}
\caption{Ablation Study Results}
\label{tab:ablation}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{R\textsuperscript{2}} \\
\midrule
Single XGBoost & 0.8699 \\
+ Feature Engineering & 0.8742 \\
+ LightGBM & 0.8789 \\
+ Tree Ensembles & 0.8821 \\
+ Intelligent Strategy Selection & 0.8853 \\
\bottomrule
\end{tabular}
\end{table}

Each component contributes incrementally to the final performance, with feature engineering and intelligent strategy selection providing the largest individual improvements.

\subsection{Limitations and Future Work}

While TurbTwin demonstrates strong performance, several limitations and opportunities for future work exist:

\begin{itemize}
    \item \textbf{Spatial Predictions}: Current implementation focuses on temporal temperature evolution; extending to full spatial field prediction would enhance utility for CFD replacement scenarios.

    \item \textbf{Online Learning}: Incorporating online learning capabilities would enable the digital twin to adapt to system drift and changing operational conditions.

    \item \textbf{Multi-Physics Integration}: Extending the framework to simultaneously predict temperature, velocity, and pressure fields would provide more comprehensive system state estimation.

    \item \textbf{Transfer Learning}: Investigating transfer learning approaches for adapting trained models to different turbulent jet configurations could improve deployment efficiency.
\end{itemize}

%===============================================================================
\section{Conclusion}
\label{sec:conclusion}
%===============================================================================

This paper presented TurbTwin, a physics-informed ensemble learning framework for digital twin-based thermal turbulent jet temperature prediction. The framework integrates CFD simulations with advanced machine learning techniques, combining heterogeneous ensemble models with physics-informed neural networks and temporal deep learning architectures.

Key contributions include:
\begin{itemize}
    \item A comprehensive digital twin architecture integrating physical and virtual components for thermal turbulent jet systems
    \item A heterogeneous ensemble framework achieving R\textsuperscript{2} = 0.8853, representing 4.8\% improvement over the best individual model
    \item Intelligent ensemble selection with five combination strategies and automated optimization
    \item Robust uncertainty quantification with 94.2\% coverage for 95\% prediction intervals
\end{itemize}

The experimental validation on real turbulent jet data demonstrates the framework's effectiveness for bridging simulation-based training with real-world prediction. The achieved performance, combined with comprehensive uncertainty quantification and computational efficiency enabling real-time inference, establishes TurbTwin as a robust foundation for physics-informed digital twin development in thermal-fluid systems.

The complete source code, trained models, and datasets are publicly available at: \url{https://github.com/nailamarir/Digitaltwin}

Future work will focus on extending the framework to spatial field prediction, incorporating online learning capabilities, and investigating transfer learning for improved generalization across different system configurations.

%===============================================================================
% ACKNOWLEDGMENT
%===============================================================================
\section*{Acknowledgment}
The authors would like to thank Effat University for providing computational resources and experimental facilities for this research.

%===============================================================================
% REFERENCES
%===============================================================================
\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}

\bibitem{glaessgen2012digital}
E. H. Glaessgen and D. S. Stargel, ``The digital twin paradigm for future NASA and U.S. Air Force vehicles,'' in \textit{Proc. 53rd AIAA/ASME/ASCE/AHS/ASC Struct., Struct. Dyn. Mater. Conf.}, 2012, p. 1818.

\bibitem{tao2017digital}
F. Tao, J. Cheng, Q. Qi, M. Zhang, H. Zhang, and F. Sui, ``Digital twin-driven product design, manufacturing and service with big data,'' \textit{Int. J. Adv. Manuf. Technol.}, vol. 94, pp. 3563--3576, 2017.

\bibitem{ritto2021digital}
T. G. Ritto and F. A. Rochinha, ``Digital twin, physics-based model, and machine learning applied to damage detection in structures,'' \textit{Mech. Syst. Signal Process.}, vol. 149, p. 107144, 2021.

\bibitem{chakraborty2021machine}
S. Chakraborty and S. Adhikari, ``Machine learning based digital twin for dynamical systems with multiple time-scales,'' \textit{Comput. Struct.}, vol. 238, p. 106282, 2021.

\bibitem{sleiti2022digital}
A. K. Sleiti, J. S. Kapat, and L. Vesely, ``Digital twin in energy industry: Proposed robust digital twin for power plant,'' \textit{Energy Rep.}, vol. 8, pp. 1718--1728, 2022.

\bibitem{molinaro2021embedding}
R. Molinaro, J. Singh, S. Catsoulis, and C. Narayanan, ``Embedding data analytics and CFD into the digital twin concept,'' \textit{Comput. Fluids}, vol. 214, p. 104759, 2021.

\bibitem{kapteyn2022data}
M. G. Kapteyn, D. J. Knezevic, D. Huynh, and K. E. Willcox, ``Data-driven physics-based digital twins via a library of component-based reduced-order models,'' \textit{Int. J. Numer. Methods Eng.}, vol. 123, pp. 3310--3332, 2022.

\bibitem{thomas2021cfd}
J. Thomas, K. Sinha, G. Shivkumar, L. Cao, and M. Funck, ``A CFD digital twin to understand miscible fluid blending,'' \textit{AAPS PharmSciTech}, vol. 22, p. 98, 2021.

\bibitem{sun2022physinet}
C. Sun and V. G. Shi, ``PhysiNet: A combination of physics-based model and neural network model for digital twins,'' \textit{Int. J. Intell. Syst.}, vol. 37, pp. 5503--5533, 2022.

\bibitem{raissi2019physics}
M. Raissi, P. Perdikaris, and G. E. Karniadakis, ``Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,'' \textit{J. Comput. Phys.}, vol. 378, pp. 686--707, 2019.

\bibitem{karniadakis2021physics}
G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang, ``Physics-informed machine learning,'' \textit{Nat. Rev. Phys.}, vol. 3, no. 6, pp. 422--440, 2021.

\bibitem{yang2021physics}
L. Yang and T. {\"O}zel, ``Physics-based simulation models for digital twin development in laser powder bed fusion,'' \textit{Int. J. Mechatronics Manuf. Syst.}, vol. 14, pp. 248--261, 2021.

\bibitem{zhou2012ensemble}
Z.-H. Zhou, \textit{Ensemble Methods: Foundations and Algorithms}. CRC Press, 2012.

\bibitem{chen2016xgboost}
T. Chen and C. Guestrin, ``XGBoost: A scalable tree boosting system,'' in \textit{Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining}, 2016, pp. 785--794.

\bibitem{ke2017lightgbm}
G. Ke \textit{et al.}, ``LightGBM: A highly efficient gradient boosting decision tree,'' in \textit{Advances in Neural Information Processing Systems}, vol. 30, 2017.

\bibitem{grieves2005product}
M. Grieves, \textit{Product Lifecycle Management: Driving the Next Generation of Lean Thinking}. McGraw-Hill, 2005.

\bibitem{kritzinger2018digital}
W. Kritzinger, M. Karner, G. Traar, J. Henjes, and W. Sihn, ``Digital twin in manufacturing: A categorical literature review and classification,'' \textit{IFAC-PapersOnLine}, vol. 51, no. 11, pp. 1016--1022, 2018.

\bibitem{jones2020characterising}
D. Jones, C. Snider, A. Nassehi, J. Yon, and B. Hicks, ``Characterising the digital twin: A systematic literature review,'' \textit{CIRP J. Manuf. Sci. Technol.}, vol. 29, pp. 36--52, 2020.

\bibitem{tao2018digital}
F. Tao and M. Zhang, ``Digital twin-based smart production management and control framework,'' \textit{Int. J. Adv. Manuf. Technol.}, vol. 96, pp. 1149--1163, 2018.

\bibitem{opoku2021digital}
D.-G. J. Opoku, S. Perera, R. Osei-Kyei, and M. Rashidi, ``Digital twin application in the construction industry: A literature review,'' \textit{J. Build. Eng.}, vol. 40, p. 102726, 2021.

\bibitem{tuhaise2023technologies}
V. V. Tuhaise, J. H. Tah, and F. H. Abanda, ``Technologies for digital twin applications in construction,'' \textit{Autom. Constr.}, vol. 147, p. 104785, 2023.

\bibitem{uhlemann2017digital}
T. H.-J. Uhlemann, C. Lehmann, and R. Steinhilper, ``The digital twin: Realizing the cyber-physical production system for Industry 4.0,'' \textit{Procedia CIRP}, vol. 61, pp. 335--340, 2017.

\bibitem{errandonea2020digital}
I. Errandonea, S. Beltr{\'a}n, and S. Arrizabalaga, ``Digital twin for maintenance: A literature review,'' \textit{Comput. Ind.}, vol. 123, p. 103316, 2020.

\bibitem{liu2018role}
Z. Liu, N. Meyendorf, and N. Mrad, ``The role of data fusion in predictive maintenance using digital twin,'' \textit{AIP Conf. Proc.}, vol. 1949, p. 020023, 2018.

\bibitem{mcclellan2022physics}
A. McClellan, J. Lorenzetti, I. Shugan, D. Freed, and S. Bieniawski, ``A physics-based digital twin for model predictive control of autonomous systems,'' \textit{Philos. Trans. R. Soc. A}, vol. 380, no. 2222, p. 20210103, 2022.

\bibitem{aivaliotis2019methodology}
P. Aivaliotis, K. Georgoulias, Z. Arkouli, and S. Makris, ``Methodology for enabling digital twin using advanced physics-based modelling in predictive maintenance,'' \textit{Procedia CIRP}, vol. 81, pp. 417--422, 2019.

\bibitem{glatt2021modeling}
M. Glatt, C. Sinnwell, L. Yi, S. Donohoe, and B. Ravani, ``Modeling and implementation of a digital twin of material flows based on physics simulation,'' \textit{J. Manuf. Syst.}, vol. 58, pp. 84--95, 2021.

\bibitem{aversano2021digital}
G. Aversano, M. Ferrarotti, and A. Parente, ``Digital twin of a combustion furnace operating in flameless conditions,'' \textit{Proc. Combust. Inst.}, vol. 38, pp. 5001--5010, 2021.

\bibitem{kapteyn2020physics}
M. G. Kapteyn and K. E. Willcox, ``From physics-based models to predictive digital twins via interpretable machine learning,'' \textit{arXiv preprint arXiv:2004.11356}, 2020.

\bibitem{nouzil2023numerical}
I. Nouzil, A. Eltaggaz, I. Deiab, and S. Pervaiz, ``Numerical CFD-FEM model for machining titanium Ti-6Al-4V with nano minimum quantity lubrication,'' \textit{J. Mater. Process. Technol.}, vol. 313, p. 117389, 2023.

\bibitem{gardner2020towards}
P. Gardner, M. Dal Borgo, V. Ruffini, A. Hughes, Y. Zhu, and E. J. Cross, ``Towards the development of an operational digital twin,'' \textit{Vibration}, vol. 3, pp. 235--265, 2020.

\bibitem{semeraro2021digital}
C. Semeraro, M. Lezoche, H. Panetto, and M. Dassisti, ``Digital twin paradigm: A systematic literature review,'' \textit{Comput. Ind.}, vol. 130, p. 103469, 2021.

\bibitem{breiman2001random}
L. Breiman, ``Random forests,'' \textit{Mach. Learn.}, vol. 45, no. 1, pp. 5--32, 2001.

\bibitem{mendes2012ensemble}
A. Mendes-Moreira, C. Soares, A. M. Jorge, and J. F. D. Sousa, ``Ensemble approaches for regression: A survey,'' \textit{ACM Comput. Surv.}, vol. 45, no. 1, pp. 1--40, 2012.

\bibitem{caruana2004ensemble}
R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes, ``Ensemble selection from libraries of models,'' in \textit{Proc. 21st Int. Conf. Mach. Learn.}, 2004, p. 18.

\end{thebibliography}

\begin{IEEEbiographynophoto}{Narjisse Kabbaj}
received her Ph.D. degree in Electrical and Computer Engineering. She is currently with the Electrical and Computer Engineering Department, Effat University, Jeddah, Saudi Arabia. Her research interests include digital twins, machine learning, and cyber-physical systems.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Naila Marir}
is with the Computer Science Department, Effat University, Jeddah, Saudi Arabia. Her research interests include machine learning, ensemble methods, and physics-informed computing.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Mohamed F. El-Amin}
(Senior Member, IEEE) is with the Energy and Technology Research Center, Effat University, Jeddah, Saudi Arabia. His research interests include computational fluid dynamics, heat transfer, and digital twin technologies for energy systems.
\end{IEEEbiographynophoto}

\end{document}
